{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+qOkOg9iUDyGW6QlkW9dP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b1590b08b684adf81260e04af717c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0d1ca7b644f4485b839c617717f3cd75",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_898d4e95c206440a93047e0c0a6e171a",
              "IPY_MODEL_41c6c614d5384b32927e7cfdae5c56c4"
            ]
          }
        },
        "0d1ca7b644f4485b839c617717f3cd75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "898d4e95c206440a93047e0c0a6e171a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e81cbf1212e145dc8a447204b1b2772a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 657434796,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 657434796,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_938c5753191f4339a058fc9fa6967efa"
          }
        },
        "41c6c614d5384b32927e7cfdae5c56c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a59a8118c7a64f7483475f9bb541b557",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 657M/657M [00:08&lt;00:00, 74.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_604db41b62364b7d92e97c8b069e0eaf"
          }
        },
        "e81cbf1212e145dc8a447204b1b2772a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "938c5753191f4339a058fc9fa6967efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a59a8118c7a64f7483475f9bb541b557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "604db41b62364b7d92e97c8b069e0eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjordana/twitter_sentiment_analysis/blob/master/roBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icSLOPoEpOV6",
        "colab_type": "text"
      },
      "source": [
        "# roBERTa's approach\n",
        "\n",
        "A new way to try to solve this [Kaggles's competition](https://www.kaggle.com/c/tweet-sentiment-extraction) is going to be thorugh the use of Facebook **roBERTa's neural network**. <br><br>\n",
        "This NN is based on Google's BERT NN, but presents some modifications. It stands for Robustly optimized BERT approach. <br>\n",
        "Its main *power* is that it dinamized the tokens, meaning that during the different *epochs* they change, getting the calculated ones. <br>\n",
        "For more info, please check this interesting [article](https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8)\n",
        "<br><br>\n",
        "\n",
        "For this first approach we are going to get our data directly from Kaggle, without any transformation.<br>\n",
        "I have learnt a lot of **roBERTa** thanks to [Chris Deotte's](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705) notebook. Besides, **hugginFace** site has eveyrthing quite well explained. I deeply recommend to have a look on their [web](https://huggingface.co/transformers/model_doc/roberta.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA-asSSyTMIs",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle's API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ngHlaFWL8po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir .kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSRkb3OyQfVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"jjordana16\",\"key\":\"6e806145f7c3fdd4c09e7299f3a70d73\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHaSVGULQ9cE",
        "colab_type": "code",
        "outputId": "23823ec8-ea9d-4e17-9e5d-4797625de3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v/content"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdrcnSQZQv1w",
        "colab_type": "code",
        "outputId": "74a38886-d6ff-4fec-94c6-407ca6316473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!kaggle competitions download -c tweet-sentiment-extraction"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content/competitions/tweet-sentiment-extraction\n",
            "  0% 0.00/1.23M [00:00<?, ?B/s]\n",
            "100% 1.23M/1.23M [00:00<00:00, 83.4MB/s]\n",
            "Downloading sample_submission.csv to /content/competitions/tweet-sentiment-extraction\n",
            "  0% 0.00/41.4k [00:00<?, ?B/s]\n",
            "100% 41.4k/41.4k [00:00<00:00, 37.7MB/s]\n",
            "Downloading test.csv to /content/competitions/tweet-sentiment-extraction\n",
            "  0% 0.00/307k [00:00<?, ?B/s]\n",
            "100% 307k/307k [00:00<00:00, 92.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D4fxDb_TPDX",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries & data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tATfE7r5i7ij",
        "colab_type": "code",
        "outputId": "313e30c1-3e65-4570-b8b1-b95c1d660ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TF version',tf.__version__)\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLwZ4tVqjBC9",
        "colab_type": "code",
        "outputId": "cf58712d-5ad4-4011-efad-0ffe8b500c47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 7.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=8eaf8c9e2c5b368605fcf69bd697b65e804c9c3f2fd0918092bfc927d1a79f2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf8UhRWhjLa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *\n",
        "import tokenizers\n",
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwjmcA1MBHCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/competitions/tweet-sentiment-extraction/train.csv.zip').fillna('')\n",
        "test = pd.read_csv('/content/competitions/tweet-sentiment-extraction/test.csv').fillna('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAVkcd7vqeSN",
        "colab_type": "text"
      },
      "source": [
        "roBERTa deals with NLP problems as any other algorithm, it need to tokenize the input data.<br>\n",
        "It allows to import our own pre-tokenized data or to use some of their tokenizers. In our case, we are going to use their data.<br>\n",
        "We will need to import some data. **HuggingFace** has all this info in his doc page. I really recommend to have a look on their [site](https://huggingface.co/transformers/_modules/transformers/tokenization_roberta.html).<br>\n",
        "For the tokenizer, we will be needing a vocab file list and the merges list.\n",
        "We will get those files by the url showed on HugginFace web. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0TzXMTZ_N3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create a folder for all roberta's data\n",
        "!mkdir /content/roberta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1RYDA4kq46T",
        "colab_type": "code",
        "outputId": "eb65d220-a9ed-4313-8696-7ec058a065c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Vocab json\n",
        "site = requests.get('https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json')\n",
        "data = site.json()\n",
        "with open('/content/roberta/roberta-base-vocab.json', 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "# Merges txt\n",
        "urllib.request.urlretrieve('https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt', \n",
        "                           \"/content/roberta/roberta-base-merges.txt\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/roberta/roberta-base-merges.txt',\n",
              " <http.client.HTTPMessage at 0x7f2108b722b0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3qcyuy_gHJ",
        "colab_type": "text"
      },
      "source": [
        "These two files will defined our tokenizer. <br>\n",
        "Our `merges.txt` file is a list in which for each alphabetic/word there is a substitution value.<br>\n",
        "For exmaple, the phrarse *What's up* would be tokenized following the merges file as *`'What', \"'s\", 'Ä up`*.\n",
        "Then, for each value that commes from the merges file, the `voca file` would substitute those tokens by index values (defined in this specific file). For example:  *`[2061, 18, 62]`*.\n",
        "<br>**Keep in mind that we have more than 50K values defined in our vocab**.\n",
        "\n",
        "`ByteLevelBPETokenizer` allows us to import this data an generae our tokenizer. We set `lowercase as true` and we set `add_prefix_space`. We basically want to add an space before the string. It is useful for the encoding/decoding.<br>\n",
        "i.e. `*tokenizer.decode(tokenizer.encode(\"Madrid\")) = \" Madrid\"*`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN3Rv4Ux-7r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining our tokenizer based on the pre-tokenizer from hugginface\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    add_prefix_space=True, \n",
        "    lowercase=True,\n",
        "    vocab_file='/content/roberta/roberta-base-vocab.json', \n",
        "    merges_file='/content/roberta/roberta-base-merges.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gIxPfI_wjPz",
        "colab_type": "text"
      },
      "source": [
        "We will stablish as **MAX_LEN** the maximum value for tweet length.<br>\n",
        "This value will be the maximum of tokens per input example. Find more info [here](https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3NHNZ8TvlqG",
        "colab_type": "code",
        "outputId": "85825a24-0b75-4e92-e806-516d9d07f6c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Max tweet length:\", max(train.text.apply(lambda x: len(x))))\n",
        "print(\"Mean tweet length\", np.mean(train.text.apply(lambda x: len(x))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max tweet length: 141\n",
            "Mean tweet length 68.32753538808632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47-bDYGwF6NB",
        "colab_type": "code",
        "outputId": "672883d4-c7c1-41b7-b3df-3853611148d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "MAX_LEN = 141\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "PAD_ID = 1\n",
        "SEED = 16\n",
        "LABEL_SMOOTHING = 0.1\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "train.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y93VseQIVIW",
        "colab_type": "text"
      },
      "source": [
        "We encode the sentiment values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRuItH0MITQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eVeXZ9XxVeK",
        "colab_type": "text"
      },
      "source": [
        "Once we have defined our tokenizer, we need to prepared our training and testing data.<br>\n",
        "We will create some empty vectors for filling in all our data.\n",
        "\n",
        "We are creating a set of arrays full of '0' and '1' with the dimension of the size of out dataset and MAX_LEN parameter.\n",
        "\n",
        "The following coding is based on [Chris Deotte's](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705) notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwY75FdQGwrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rowsFile = train.shape[0]\n",
        "\n",
        "input_ids = np.ones((rowsFile,MAX_LEN),dtype='int32')\n",
        "attention_mask =  np.zeros((rowsFile,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((rowsFile,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((rowsFile,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((rowsFile,MAX_LEN),dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isaLt2NGQqpC",
        "colab_type": "text"
      },
      "source": [
        "We will proceed by: <br>\n",
        "1. Selecting all our tweet and selection texts. <br>\n",
        "The idea is to find the index of the `selected_text` in the `tweet` text. Like this, in a full zero vector X of length equalt to the tweet's length, we will fulfill 1 for determining where the selected_test is.<br>\n",
        "Then we codify our resulting vector by substituting each word by it's corresponding token.<br>\n",
        "       i.e.: tweet: \"I love mondays\"; len(text): 14; selected_text: \"love\"; len(selected_text): 4\n",
        "             X =       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "             index where selected_text start: 2\n",
        "             result:   [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "             enc:      ['Ä i', 'Ä love', 'Ä m', 'ond', 'ays']\n",
        "2. Each enc list contains all the words codified. By selecting `enc.id` we can access to their corresponding index. <br>\n",
        "In this part we basically get the length for each token. We decodify our tokens thorugh their index, and we get back the words. We initiate a new list that is going to contain the starting and ending positions of the whole tweet.\n",
        "       i.e.: enc.id:               [939, 657, 475, 2832, 4113]\n",
        "             enc decodified:       [' i', ' love', ' m', 'ond', 'ays']\n",
        "             length of each token: [(0, 2), (2, 7), (7, 9), (9, 12), (12, 15)]\n",
        "3. Now we loop in each __offsert__. Basically, we are getting the length of each token, and by knwoing the length we can select the sub vector (for that length) in our vector **chars** (in the example above would be result). Therefore, we sum all the partitions, and if the sum is higger thant 1 we append to our list **token**.\n",
        "        i.e.: length of each token: [(0, 2), (2, 7), (7, 9), (9, 12), (12, 15)]\n",
        "              result:               [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "              We get the partitions. In this example we would get the two first one:\n",
        "              Partition result[0:2] [0, 0 ,1] ====> sum(result[0:2]) > 1 ====> add to token the sum value (1)\n",
        "              Partition result[2:7] [1, 1, 1, 1, 0, 0] ====> sum(result[2:7]) > 1 ====> add to token the sum value (4)\n",
        "4. Finally, we overwrite all our previous created lists (in the previous cell) with the corresponding data calculates.\n",
        "             \n",
        "          \n",
        "        \n",
        "        \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuTZJm02IMBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for val in range(rowsFile):\n",
        "  # FIND OVERLAP\n",
        "  text1 = \" \" + \" \".join(train.loc[val,'text'].split())\n",
        "  text2 = \" \".join(train.loc[val,'selected_text'].split())\n",
        "\n",
        "  idx = text1.find(text2) # We find index where our selected_word starts\n",
        "  chars = np.zeros((len(text1))) # We create a vector of 0s with the length of text\n",
        "  chars[idx:idx+len(text2)] = 1 # Fullfill the vector with 1 when it coincides\n",
        "  if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "  enc = tokenizer.encode(text1) \n",
        "\n",
        "  # ID_OFFSETS\n",
        "  offsets = []\n",
        "  idx=0\n",
        "  for t in enc.ids:\n",
        "    w = tokenizer.decode([t])\n",
        "    offsets.append((idx,idx+len(w)))\n",
        "    idx += len(w)\n",
        "\n",
        "  # START END TOKENS\n",
        "  toks = []\n",
        "  for i,(a,b) in enumerate(offsets):  \n",
        "    sm = np.sum(chars[a:b])\n",
        "    if sm>0: toks.append(i) \n",
        "        \n",
        "  s_tok = sentiment_id[train.loc[val,'sentiment']]\n",
        "  input_ids[val,:len(enc.ids) + 5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "  attention_mask[val,:len(enc.ids)+5] = 1\n",
        "  if len(toks)>0:\n",
        "    start_tokens[val,toks[0]+1] = 1\n",
        "    end_tokens[val,toks[-1]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnuXHLWJV1nC",
        "colab_type": "text"
      },
      "source": [
        "We have now to perform in the same way with the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FyG58I6N4CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rowsFile = test.shape[0]\n",
        "input_ids_t = np.ones((rowsFile,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((rowsFile,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((rowsFile,MAX_LEN),dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgmOf1QpWTjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for val in range(rowsFile):   \n",
        "  # INPUT_IDS\n",
        "  text1 = \" \"+\" \".join(test.loc[val,'text'].split())\n",
        "  enc = tokenizer.encode(text1)                \n",
        "  s_tok = sentiment_id[test.loc[val,'sentiment']]\n",
        "  input_ids_t[val,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "  attention_mask_t[val,:len(enc.ids)+5] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_x3fCzyyyQb",
        "colab_type": "text"
      },
      "source": [
        "Our data is preapred. All our tweets and selected_text have been tokenized.<br>\n",
        "Now, we must define our model. We will be using **HuggingFace** [configuration](https://github.com/huggingface/transformers/blob/master/src/transformers/configuration_roberta.py) and [pretrained models](https://huggingface.co/transformers/_modules/transformers/modeling_tf_roberta.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmPGzpyaZNdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config json\n",
        "site = requests.get('https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json')\n",
        "data = site.json()\n",
        "with open('/content/roberta/roberta-base-config.json', 'w') as f:\n",
        "    json.dump(data, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddVXwGt4LczW",
        "colab_type": "text"
      },
      "source": [
        "`build model` function consists on: <br>\n",
        "defining the **configuration** and **pretrained model** that we want to use creating the different layers for our model <br>\n",
        "`dropoutLayer` it's active during the training. We stablis a rate at each step. Inputs that are different to 0 are scaled up by 1/(1 - rate). It's applied directly to the build_model(). <br>\n",
        "`Conv1D` creates a convolution kernel. The first parameter, _filter_, defines the dimension of the output; the second defines the dimension of the 1D convolution window.<br>\n",
        "`Flatten` flattens the input matriz size <br>\n",
        "`Activation` in our case we use _softmax__ to activate an output. This function transforms the vector into a vector of categorical probabilities (probabilistic distribution).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbAv50Os35fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  config = RobertaConfig.from_pretrained('/content/roberta/roberta-base-config.json')\n",
        "  bert_model = TFRobertaModel.from_pretrained(\"https://cdn.huggingface.co/roberta-base-tf_model.h5\", config=config)\n",
        "\n",
        "  ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32) #ids layer\n",
        "  att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32) #attention mask\n",
        "  tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32) # token layer\n",
        "  \n",
        "  x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "  \n",
        "  # First output\n",
        "  x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "  x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "  x1 = tf.keras.layers.Flatten()(x1)\n",
        "  x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "  # Second output\n",
        "  x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "  x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "  x2 = tf.keras.layers.Flatten()(x2)\n",
        "  x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=4e-5)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5x8EJRnUHgU",
        "colab_type": "text"
      },
      "source": [
        "To measure our score we will use, as we have done before in other notebooks, the `jaccard score`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MoGh02i4nvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccardScore(str1, str2): \n",
        "  wordA = set(str1.lower().split())\n",
        "  wordB = set(str1.lower().split())\n",
        "  if (len(wordA)==0) & (len(wordB)==0): \n",
        "    return 0.5\n",
        "  intersect = wordA.intersection(wordB)\n",
        "  return float(len(intersect)) / (len(wordA) + len(wordB) - len(intersect))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Egs7Cll4sxq",
        "colab_type": "code",
        "outputId": "ee9ece84-c642-4f9a-aa78-9a10a7ebcf46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b1590b08b684adf81260e04af717c91",
            "0d1ca7b644f4485b839c617717f3cd75",
            "898d4e95c206440a93047e0c0a6e171a",
            "41c6c614d5384b32927e7cfdae5c56c4",
            "e81cbf1212e145dc8a447204b1b2772a",
            "938c5753191f4339a058fc9fa6967efa",
            "a59a8118c7a64f7483475f9bb541b557",
            "604db41b62364b7d92e97c8b069e0eaf"
          ]
        }
      },
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccardScore(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b1590b08b684adf81260e04af717c91",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=657434796.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "687/687 [==============================] - ETA: 0s - loss: 2.2309 - activation_loss: 1.1104 - activation_1_loss: 1.1205\n",
            "Epoch 00001: val_loss improved from inf to 1.77034, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 382s 556ms/step - loss: 2.2309 - activation_loss: 1.1104 - activation_1_loss: 1.1205 - val_loss: 1.7703 - val_activation_loss: 0.9024 - val_activation_1_loss: 0.8679\n",
            "Epoch 2/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 1.6430 - activation_loss: 0.8482 - activation_1_loss: 0.7948\n",
            "Epoch 00002: val_loss improved from 1.77034 to 1.64705, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 380s 553ms/step - loss: 1.6430 - activation_loss: 0.8482 - activation_1_loss: 0.7948 - val_loss: 1.6470 - val_activation_loss: 0.8577 - val_activation_1_loss: 0.7894\n",
            "Epoch 3/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 1.5020 - activation_loss: 0.7749 - activation_1_loss: 0.7271\n",
            "Epoch 00003: val_loss improved from 1.64705 to 1.62312, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 380s 553ms/step - loss: 1.5020 - activation_loss: 0.7749 - activation_1_loss: 0.7271 - val_loss: 1.6231 - val_activation_loss: 0.8419 - val_activation_1_loss: 0.7812\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 181ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7068776569981835\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.1740 - activation_loss: 1.0819 - activation_1_loss: 1.0922\n",
            "Epoch 00001: val_loss improved from inf to 1.68696, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 396s 575ms/step - loss: 2.1740 - activation_loss: 1.0819 - activation_1_loss: 1.0922 - val_loss: 1.6870 - val_activation_loss: 0.8868 - val_activation_1_loss: 0.8002\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6461 - activation_loss: 0.8512 - activation_1_loss: 0.7949\n",
            "Epoch 00002: val_loss improved from 1.68696 to 1.64241, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 393s 571ms/step - loss: 1.6461 - activation_loss: 0.8512 - activation_1_loss: 0.7949 - val_loss: 1.6424 - val_activation_loss: 0.8434 - val_activation_1_loss: 0.7990\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.5045 - activation_loss: 0.7857 - activation_1_loss: 0.7188\n",
            "Epoch 00003: val_loss did not improve from 1.64241\n",
            "688/688 [==============================] - 391s 569ms/step - loss: 1.5045 - activation_loss: 0.7857 - activation_1_loss: 0.7188 - val_loss: 1.6517 - val_activation_loss: 0.8563 - val_activation_1_loss: 0.7954\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 182ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 180ms/step\n",
            ">>>> FOLD 2 Jaccard = 0.6960016422155905\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.5318 - activation_loss: 1.1886 - activation_1_loss: 1.3431\n",
            "Epoch 00001: val_loss improved from inf to 1.82591, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 395s 575ms/step - loss: 2.5318 - activation_loss: 1.1886 - activation_1_loss: 1.3431 - val_loss: 1.8259 - val_activation_loss: 0.8856 - val_activation_1_loss: 0.9403\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.8661 - activation_loss: 0.9181 - activation_1_loss: 0.9479\n",
            "Epoch 00002: val_loss improved from 1.82591 to 1.74536, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 393s 571ms/step - loss: 1.8661 - activation_loss: 0.9181 - activation_1_loss: 0.9479 - val_loss: 1.7454 - val_activation_loss: 0.8780 - val_activation_1_loss: 0.8674\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6682 - activation_loss: 0.8477 - activation_1_loss: 0.8204\n",
            "Epoch 00003: val_loss did not improve from 1.74536\n",
            "688/688 [==============================] - 391s 569ms/step - loss: 1.6682 - activation_loss: 0.8477 - activation_1_loss: 0.8204 - val_loss: 1.8911 - val_activation_loss: 0.8916 - val_activation_1_loss: 0.9995\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 181ms/step\n",
            ">>>> FOLD 3 Jaccard = 0.6896376621768214\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.2840 - activation_loss: 1.1269 - activation_1_loss: 1.1571\n",
            "Epoch 00001: val_loss improved from inf to 1.67751, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 395s 575ms/step - loss: 2.2840 - activation_loss: 1.1269 - activation_1_loss: 1.1571 - val_loss: 1.6775 - val_activation_loss: 0.8427 - val_activation_1_loss: 0.8348\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6929 - activation_loss: 0.8587 - activation_1_loss: 0.8342\n",
            "Epoch 00002: val_loss improved from 1.67751 to 1.66774, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 393s 572ms/step - loss: 1.6929 - activation_loss: 0.8587 - activation_1_loss: 0.8342 - val_loss: 1.6677 - val_activation_loss: 0.8382 - val_activation_1_loss: 0.8296\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.5630 - activation_loss: 0.7949 - activation_1_loss: 0.7682\n",
            "Epoch 00003: val_loss did not improve from 1.66774\n",
            "688/688 [==============================] - 392s 569ms/step - loss: 1.5630 - activation_loss: 0.7949 - activation_1_loss: 0.7682 - val_loss: 1.7419 - val_activation_loss: 0.8477 - val_activation_1_loss: 0.8941\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 183ms/step\n",
            ">>>> FOLD 4 Jaccard = 0.6970317977363176\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.1280 - activation_loss: 1.0822 - activation_1_loss: 1.0458\n",
            "Epoch 00001: val_loss improved from inf to 1.67650, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 396s 575ms/step - loss: 2.1280 - activation_loss: 1.0822 - activation_1_loss: 1.0458 - val_loss: 1.6765 - val_activation_loss: 0.8799 - val_activation_1_loss: 0.7966\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6246 - activation_loss: 0.8385 - activation_1_loss: 0.7861\n",
            "Epoch 00002: val_loss improved from 1.67650 to 1.61244, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 393s 572ms/step - loss: 1.6246 - activation_loss: 0.8385 - activation_1_loss: 0.7861 - val_loss: 1.6124 - val_activation_loss: 0.8455 - val_activation_1_loss: 0.7670\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.4596 - activation_loss: 0.7561 - activation_1_loss: 0.7035\n",
            "Epoch 00003: val_loss improved from 1.61244 to 1.60196, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 393s 571ms/step - loss: 1.4596 - activation_loss: 0.7561 - activation_1_loss: 0.7035 - val_loss: 1.6020 - val_activation_loss: 0.8265 - val_activation_1_loss: 0.7755\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 181ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 180ms/step\n",
            ">>>> FOLD 5 Jaccard = 0.711529026639255\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbOd1PZgR7U_",
        "colab_type": "code",
        "outputId": "049ee77a-09bb-42c8-faf4-3727ddbb55ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> OVERALL 5Fold CV Jaccard = 0.7002155571532336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZd3k7fJR79l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    all.append(st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qBcb6QHR_d7",
        "colab_type": "code",
        "outputId": "9a845017-8391-45d4-8b79-54bf005099fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "source": [
        "test['selected_text'] = all\n",
        "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
        "pd.set_option('max_colwidth', 60)\n",
        "test.sample(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>b3eadfc565</td>\n",
              "      <td>Folks thought it was hilarious when I told them the sto...</td>\n",
              "      <td>positive</td>\n",
              "      <td>hilarious</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>e8ce4f8bdc</td>\n",
              "      <td>ohh snapp, have fun</td>\n",
              "      <td>neutral</td>\n",
              "      <td>ohh snapp, have fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2853</th>\n",
              "      <td>8244eb7ae9</td>\n",
              "      <td>omg, NO ICECREAM</td>\n",
              "      <td>negative</td>\n",
              "      <td>omg, no icecream</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2782</th>\n",
              "      <td>aa2c27ea07</td>\n",
              "      <td>I like Rio Ferdinand - when he`s wearing an England jersey.</td>\n",
              "      <td>positive</td>\n",
              "      <td>like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3dcf4f7e13</td>\n",
              "      <td>... need retail therapy, bad. AHHH.....gimme money geebus</td>\n",
              "      <td>negative</td>\n",
              "      <td>bad.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>abda0ac082</td>\n",
              "      <td>is fixin to clean the house for my mom for mother`s day</td>\n",
              "      <td>positive</td>\n",
              "      <td>is fixin to clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2550</th>\n",
              "      <td>b8e35a07a2</td>\n",
              "      <td>bout to go to bed... pretty good day for a Monday.</td>\n",
              "      <td>positive</td>\n",
              "      <td>pretty good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>d7b1ac80c4</td>\n",
              "      <td>Good morning!  Just took the longest shower ive ever tak...</td>\n",
              "      <td>positive</td>\n",
              "      <td>good morning!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>bab26fdc24</td>\n",
              "      <td>pansy  wtf codeh?!</td>\n",
              "      <td>negative</td>\n",
              "      <td>wtf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>4805e663c0</td>\n",
              "      <td>Have been writing since 6pm &amp; I only have 300 words. Can...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>have been writing since 6pm &amp; i only have 300 words. ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3073</th>\n",
              "      <td>8230f9ad74</td>\n",
              "      <td>makin tea an its stressin me out</td>\n",
              "      <td>negative</td>\n",
              "      <td>stressin me out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1560</th>\n",
              "      <td>d3a576d221</td>\n",
              "      <td>you asked about my SF schedule, dahling...maybe next time</td>\n",
              "      <td>neutral</td>\n",
              "      <td>you asked about my sf schedule, dahling...maybe next time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>720</th>\n",
              "      <td>15fb68bf9f</td>\n",
              "      <td>All of a sudden I`m craving broccoli and cheese soup rea...</td>\n",
              "      <td>negative</td>\n",
              "      <td>bad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2123</th>\n",
              "      <td>3da6270b02</td>\n",
              "      <td>I hide my berry like a slave REGULARLY only today I was...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>i hide my berry like a slave regularly only today i was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2528</th>\n",
              "      <td>333f3d42e2</td>\n",
              "      <td>aww bless  haha your cute Tom</td>\n",
              "      <td>positive</td>\n",
              "      <td>aww bless haha your cute tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3468</th>\n",
              "      <td>885b8f47f7</td>\n",
              "      <td>yes sir i sure did.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>yes sir i sure did.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2549</th>\n",
              "      <td>0fb19285b2</td>\n",
              "      <td>HEY GUYS IT`S WORKING NO NEED TO WORRY. i have tooo many...</td>\n",
              "      <td>positive</td>\n",
              "      <td>hey guys it`s working no need to worry.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3269</th>\n",
              "      <td>49096c54e2</td>\n",
              "      <td>My 100th post- dedicated to Sarah,sunny weather,and my l...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>my 100th post- dedicated to sarah,sunny weather,and my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2551</th>\n",
              "      <td>df8b63c379</td>\n",
              "      <td>_18 i emailed you the link, pretty sad uh?  RIP Jessie K...</td>\n",
              "      <td>negative</td>\n",
              "      <td>pretty sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2707</th>\n",
              "      <td>a30a9b4003</td>\n",
              "      <td>Though it was a cold that would go away, turns out it`s...</td>\n",
              "      <td>negative</td>\n",
              "      <td>worse and worse by the day.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2651</th>\n",
              "      <td>0d63dff2e5</td>\n",
              "      <td>you don`t even care about there, their, and they`re.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>you don`t even care about there, their, and they`re.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>a22d319e5f</td>\n",
              "      <td>- That Jasper clip is my first 'favorite' Twitter message.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>- that jasper clip is my first 'favorite' twitter message.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1289</th>\n",
              "      <td>53dbbbdf7d</td>\n",
              "      <td>_ember yay! thanks.  cover-i sad ang alist please. )</td>\n",
              "      <td>neutral</td>\n",
              "      <td>_ember yay! thanks. cover-i sad ang alist please. )</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2483</th>\n",
              "      <td>95dbb10a09</td>\n",
              "      <td>Driving home to trade cars  hopefully it makes it! http:...</td>\n",
              "      <td>positive</td>\n",
              "      <td>hopefully</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2599</th>\n",
              "      <td>12ceb7565a</td>\n",
              "      <td>Outlook not so good</td>\n",
              "      <td>negative</td>\n",
              "      <td>not so good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ...                                                selected_text\n",
              "126   b3eadfc565  ...                                                    hilarious\n",
              "1075  e8ce4f8bdc  ...                                          ohh snapp, have fun\n",
              "2853  8244eb7ae9  ...                                             omg, no icecream\n",
              "2782  aa2c27ea07  ...                                                         like\n",
              "22    3dcf4f7e13  ...                                                         bad.\n",
              "240   abda0ac082  ...                                            is fixin to clean\n",
              "2550  b8e35a07a2  ...                                                  pretty good\n",
              "419   d7b1ac80c4  ...                                                good morning!\n",
              "1533  bab26fdc24  ...                                                          wtf\n",
              "2488  4805e663c0  ...   have been writing since 6pm & i only have 300 words. ca...\n",
              "3073  8230f9ad74  ...                                              stressin me out\n",
              "1560  d3a576d221  ...    you asked about my sf schedule, dahling...maybe next time\n",
              "720   15fb68bf9f  ...                                                       bad...\n",
              "2123  3da6270b02  ...   i hide my berry like a slave regularly only today i was...\n",
              "2528  333f3d42e2  ...                                 aww bless haha your cute tom\n",
              "3468  885b8f47f7  ...                                          yes sir i sure did.\n",
              "2549  0fb19285b2  ...                      hey guys it`s working no need to worry.\n",
              "3269  49096c54e2  ...   my 100th post- dedicated to sarah,sunny weather,and my ...\n",
              "2551  df8b63c379  ...                                                   pretty sad\n",
              "2707  a30a9b4003  ...                                  worse and worse by the day.\n",
              "2651  0d63dff2e5  ...         you don`t even care about there, their, and they`re.\n",
              "1284  a22d319e5f  ...   - that jasper clip is my first 'favorite' twitter message.\n",
              "1289  53dbbbdf7d  ...          _ember yay! thanks. cover-i sad ang alist please. )\n",
              "2483  95dbb10a09  ...                                                    hopefully\n",
              "2599  12ceb7565a  ...                                                  not so good\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}